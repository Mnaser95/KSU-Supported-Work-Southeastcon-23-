{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b4f57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.fft import fft, fftfreq, ifft\n",
    "import scipy.fftpack\n",
    "from matplotlib import *\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import *\n",
    "from scipy import signal\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import mne\n",
    "from mne.time_frequency import psd_welch\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "import mne\n",
    "from mne import Epochs, pick_types, events_from_annotations\n",
    "from mne.channels import make_standard_montage\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "from sklearn.metrics import r2_score\n",
    "from mne.decoding import CSP \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import svm\n",
    "import pywt\n",
    "from sklearn.model_selection import KFold\n",
    "from mne.preprocessing import (ICA, create_eog_epochs, create_ecg_epochs,corrmap)\n",
    "from mne import compute_covariance\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# Inputs \n",
    "\n",
    "labels={ \"1\"   : 769 # MI-left\n",
    "        ,\"2\"   : 770 # MI-right\n",
    "        ,\"3\"   : 800 # Rest\n",
    "        ,\"4\"   : 771 # MI-feet\n",
    "        ,\"5\"   : 780 # Idle\n",
    "        ,\"6\"   : 786 # Cross\n",
    "       }\n",
    "\n",
    "sampling_freq = 500\n",
    "needed_subjects=20\n",
    "num_channels= 28\n",
    "T=1/sampling_freq\n",
    "\n",
    "channels_mne=[\"F3\",\"F1\",\"FZ\",\"F2\",\"F4\",\"FC5\",\"FC3\",\"FC1\",\"FCZ\",\"FC2\",\"FC4\",\"FC6\",\"C5\",\"C3\",\"C1\",\"CZ\",\"C2\",\"C4\",\"C6\",\"CP5\",\"CP3\",\"CP1\",\"CPZ\",\"CP2\",\"CP4\",\"CP6\",\"hEOG\",\"vEOG\"]\n",
    "info = mne.create_info(ch_names=channels_mne, sfreq=sampling_freq, ch_types='eeg')\n",
    "\n",
    "sub_dict={}\n",
    "run_dict={}\n",
    "master_directory=str(r\"C:\\Users\\mnaser1\\OneDrive - Kennesaw State University\\Desktop\\PhD-S4\\Research\\Proposals\\NSF\\Data\\1\\files\")\n",
    "for level_one in os.listdir(master_directory):  \n",
    "    sub_num=int(level_one)\n",
    "    secondary_directory=master_directory+str(\"\\\\\")+str(level_one)\n",
    "    run_num=1\n",
    "    for level_two in os.listdir(secondary_directory):\n",
    "        current_file=secondary_directory+str(\"\\\\\")+str(level_two)\n",
    "        orig=dict(np.load(current_file))\n",
    "        data = orig['signal']\n",
    "        index = orig['MarkOnSignal'] \n",
    "        num_rows,num_columns=data.shape\n",
    "        data=data[:,:28]         # Remove unwanted channels  \n",
    "        data=np.insert(data,0,0, axis=1)\n",
    "        for i in range(0, len(index)):\n",
    "            my_index=index[i][0]\n",
    "            my_label=index[i][1]\n",
    "            data[my_index-1][0]=my_label    \n",
    "        run_dict[run_num]=data\n",
    "        #print(\"sub\",sub_num,\"run\",run_num,\"is ready\")\n",
    "        run_num=run_num+1\n",
    "    sub_dict[sub_num]=run_dict\n",
    "    run_dict={}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca5310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting events and labels\n",
    "\n",
    "\n",
    "MI_c_dict={}\n",
    "run_c_dict={}\n",
    "sub_c_dict={}\n",
    "MI_events={}\n",
    "run_events={}\n",
    "sub_events={}\n",
    "tasks=np.ndarray(shape=(1,num_channels))\n",
    "single_events=np.ndarray(shape=(1,3))\n",
    "\n",
    "for sub in range(1,needed_subjects+1):\n",
    "    num_runs=len(sub_dict[sub])\n",
    "    for run in range(1,num_runs+1):\n",
    "        for MI in range (1,4):\n",
    "            if MI==1 or MI==2:\n",
    "                num_samples=sampling_freq*5\n",
    "            else:\n",
    "                num_samples=sampling_freq*3\n",
    "            current_MI=np.where(sub_dict[sub][run][:,0] ==labels[str(MI)]) ; current_MI=list(current_MI[0])  \n",
    "            for l in current_MI:\n",
    "                tasks=np.append(tasks,sub_dict[sub][run][l:l+num_samples,1:num_channels+1],axis=0)   \n",
    "            tasks=np.delete(tasks, 0, axis=0)\n",
    "            MI_c_dict[MI]=tasks\n",
    "            tasks=np.ndarray(shape=(1,num_channels))\n",
    "        run_c_dict[run]= MI_c_dict\n",
    "        MI_c_dict={}\n",
    "    sub_c_dict[sub]=run_c_dict\n",
    "    run_c_dict={}\n",
    "    #print(\"sub \",sub, \"is done\")\n",
    "            \n",
    "sub_data_run={}\n",
    "sub_data={}\n",
    "num_samples=5*sampling_freq\n",
    "for sub in range(1,needed_subjects+1):\n",
    "    num_runs=len(sub_dict[sub])\n",
    "    for run in range(1,num_runs+1):\n",
    "        sub_data_run[run]=np.concatenate((sub_c_dict[sub][run][1],sub_c_dict[sub][run][2]),axis=0)\n",
    "        experiment_length1=len(sub_c_dict[sub][run][1])\n",
    "        experiment_length2=len(sub_c_dict[sub][run][2])\n",
    "        for c in range (0,experiment_length1,num_samples):\n",
    "            my_index=int(c)\n",
    "            temp_events=[my_index,0,1]\n",
    "            temp_events=np.reshape(temp_events,(1,3))\n",
    "            single_events=np.append(single_events,temp_events,axis=0)\n",
    "        for c in range (experiment_length1,experiment_length1+experiment_length2,num_samples):\n",
    "            my_index=int(c)\n",
    "            temp_events=[my_index,0,2]\n",
    "            temp_events=np.reshape(temp_events,(1,3))\n",
    "            single_events=np.append(single_events,temp_events,axis=0)\n",
    "        single_events=np.delete(single_events, 0, axis=0) \n",
    "        run_events[run]= single_events      \n",
    "        single_events=np.ndarray(shape=(1,3))  \n",
    "    sub_events[sub]=run_events\n",
    "    run_events={}\n",
    "    sub_data[sub]=sub_data_run\n",
    "    sub_data_run={}\n",
    "    #print(\"sub \",sub, \"is done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25780f0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_random_recording(sub,run,MI,my_duration):\n",
    "    file_size=len(sub_c_dict[sub][run][MI])\n",
    "    len_required=my_duration*sampling_freq\n",
    "    random_beg=random.randint(0,file_size-len_required)\n",
    "    det_end=random_beg+len_required\n",
    "    random_rec=sub_c_dict[sub][run][MI][random_beg:det_end]\n",
    "    return(random_rec)\n",
    "\n",
    "psd_current=np.ndarray(shape=(1,4))\n",
    "psd_MI={}\n",
    "psd_sub={}\n",
    "for sub in range(1,needed_subjects+1):\n",
    "    num_runs=len(sub_dict[sub])\n",
    "    for MI in range (1,4):\n",
    "        psd_current=np.ndarray(shape=(1,4))\n",
    "        for run in range(1,num_runs+1):\n",
    "            if MI==1 or MI==2:\n",
    "                my_duration=int(min(60,len(sub_c_dict[sub][run][MI])/sampling_freq))\n",
    "            else:\n",
    "                my_duration=int(min(120,len(sub_c_dict[sub][run][MI])/sampling_freq))\n",
    "            values=generate_random_recording(sub,run,MI,my_duration)\n",
    "            values=transpose(values)\n",
    "            raw=mne.io.RawArray(values, info)\n",
    "            raw.filter(6, 35, fir_design='firwin', skip_by_annotation='edge')\n",
    "            my_event=mne.make_fixed_length_events(raw, id=1, start=0, stop=None, duration=my_duration, first_samp=False, overlap=0.0)\n",
    "            epochs = mne.Epochs(raw, \n",
    "                            events=my_event, \n",
    "                            event_id=None, \n",
    "                            tmin=0, \n",
    "                            tmax=my_duration-.002, \n",
    "                            baseline=None, \n",
    "                            preload=True)\n",
    "\n",
    "            my_psd = psd_welch(epochs, tmin=0, tmax=my_duration, picks=channels_mne,\n",
    "                                fmin=0, fmax=30, n_per_seg=None,n_fft=1000,average=\"mean\") # the avg is for each 2 hz with its representing samples. my_psd will get three arrays, the first for c3 and the second for c4 and the third is the frequencies (the x-axis). each array has the length of range of frequencies/2hz (50/2 ) and plus 1 for unaggregated segments .\n",
    "            my_freqs=my_psd[1]\n",
    "            my_psd=my_psd[0]  #to remove the extra array\n",
    "            my_psd=my_psd[0]  #to remove the extra array\n",
    "\n",
    "            my_psd_ch=np.ndarray(shape=(14,61))\n",
    "            my_psd_ch_alpha=np.ndarray(shape=(14,1))\n",
    "            my_psd_ch_beta=np.ndarray(shape=(14,1))\n",
    "            for ch in range(5,19):\n",
    "                my_psd_ch[ch-5]=my_psd[ch,:]\n",
    "                my_psd_ch_alpha[ch-5]=np.average(my_psd_ch[ch-5][14:28])\n",
    "                my_psd_ch_beta[ch-5]=np.average(my_psd_ch[ch-5][29:60])\n",
    "            my_psd_even_alpha=(my_psd_ch_alpha[4]+my_psd_ch_alpha[5]+my_psd_ch_alpha[6]+my_psd_ch_alpha[11]+my_psd_ch_alpha[12]+my_psd_ch_alpha[13])/6\n",
    "            my_psd_even_beta=(my_psd_ch_beta[4]+my_psd_ch_beta[5]+my_psd_ch_beta[6]+my_psd_ch_beta[11]+my_psd_ch_beta[12]+my_psd_ch_beta[13])/6\n",
    "            my_psd_odd_alpha=(my_psd_ch_alpha[0]+my_psd_ch_alpha[1]+my_psd_ch_alpha[2]+my_psd_ch_alpha[7]+my_psd_ch_alpha[8]+my_psd_ch_alpha[9])/6\n",
    "            my_psd_odd_beta=(my_psd_ch_beta[0]+my_psd_ch_beta[1]+my_psd_ch_beta[2]+my_psd_ch_beta[7]+my_psd_ch_beta[8]+my_psd_ch_beta[9])/6\n",
    "            psd_now=np.array([my_psd_odd_alpha,my_psd_even_alpha, my_psd_odd_beta,my_psd_even_beta])\n",
    "            psd_now=np.reshape(psd_now,(1,4))\n",
    "            psd_current=np.append(psd_current,psd_now,axis=0)\n",
    "        psd_current=np.delete(psd_current, 0, axis=0)    \n",
    "        psd_MI[MI]=psd_current\n",
    "    psd_sub[sub]=psd_MI\n",
    "    psd_MI={}\n",
    "    \n",
    "    print(\"sub:\", sub,\"is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c8888",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sub in range(1, needed_subjects+1):\n",
    "    for MI in range(1,4):\n",
    "        num_runs=len(sub_dict[sub])\n",
    "        if MI==1:\n",
    "            f=\"left\"\n",
    "        elif MI==2:\n",
    "            f=\"right\"\n",
    "        else: \n",
    "            f=\"rest\"\n",
    "        print(\"sub:\",sub,\"MI\",f,\"------------------------------------------------------\")\n",
    "        print(psd_sub[sub][MI])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb277561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a1dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936599f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c418efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8051ae40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ca362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15883b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104adae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa8ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be1c9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31443e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f9bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9939c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094f4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb19e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e12435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6f204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3296e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d069ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c660df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243a4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900b5f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661b60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac130989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5680adf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f59b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705bdc2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84aa38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d868eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f050b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220195e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "channels_mne_acc=[\"F3\",\"F1\",\"Fz\",\"F2\",\"F4\",\"FC5\",\"FC3\",\"FC1\",\"FCz\",\"FC2\",\"FC4\",\"FC6\",\"C5\",\"C3\",\"C1\",\"Cz\",\"C2\",\"C4\",\"C6\",\"CP5\",\"CP3\",\"CP1\",\"CPz\",\"CP2\",\"CP4\",\"CP6\",\"hEOG\",\"vEOG\"] \n",
    "channels_types=['eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eeg','eog','eog'] \n",
    "chosen_channels_mne=[\"C1\",\"C3\",\"C5\",\"C2\",\"C4\",\"C6\",\"Cz\",\"FC1\",\"FC3\",\"FC5\",\"FC2\",\"FC4\",\"FC6\",\"FCz\"] #chosen_channels_mne=[\"C1\",\"C3\",\"C5\",\"FC1\",\"FC3\",\"FC5\"] #chosen_channels_mne=[\"C2\",\"C4\",\"C6\",\"FC2\",\"FC4\",\"FC6\"]\n",
    "info = mne.create_info(ch_names=channels_mne_acc, sfreq=sampling_freq, ch_types=channels_types) \n",
    "info.set_montage('standard_1020')#,on_missing='ignore')\n",
    "\n",
    "\n",
    "def data_scaling(data): \n",
    "    scaler = StandardScaler() # for each channel, the std becomes 1 and the average is 0 # better to be replaced with noise covarience at rest EEG: \n",
    "    #noise_covs = compute_covariance(data,tmin=None, tmax=None, method='auto',return_estimators=True, n_jobs=None,projs=None, rank=None, verbose=True) \n",
    "    data_scaled= scaler.fit_transform(data) \n",
    "    return(data_scaled)\n",
    "\n",
    "def find_PCA_components(data_scaled): # needed for ICA \n",
    "    pca=PCA(num_channels,whiten=True) \n",
    "    pca.fit(data_scaled)\n",
    "\n",
    "    cum_exp_var = []\n",
    "    var_exp = 0\n",
    "    c=1\n",
    "    for i in pca.explained_variance_ratio_:\n",
    "        var_exp += i\n",
    "        cum_exp_var.append(var_exp)\n",
    "        if var_exp>0.95:\n",
    "            num_PCA_comp=c\n",
    "            break\n",
    "        c=c+1\n",
    "    return(num_PCA_comp)\n",
    "\n",
    "def define_raw(data_scaled):\n",
    "    data_transposed=transpose(data_scaled) \n",
    "    raw =mne.io.RawArray(data_transposed, info) \n",
    "    raw.set_eeg_reference('average',ch_type='eeg') \n",
    "    return(raw)\n",
    "\n",
    "\n",
    "def ICA_components(raw,num_PCA_comp): \n",
    "    raw.filter(1, 100, fir_design='firwin', skip_by_annotation='edge')\n",
    "    ica = ICA(n_components=num_PCA_comp, max_iter='auto', random_state=97)\n",
    "    ica.fit(raw,picks=[\"eeg\",\"eog\"])\n",
    "    ica.plot_components()\n",
    "    eog_indices, eog_scores = ica.find_bads_eog(raw)\n",
    "    ica.plot_scores(eog_scores)\n",
    "\n",
    "    for i in range(0,2):\n",
    "        for ii in range(0,num_PCA_comp):\n",
    "            if abs(eog_scores[i][ii])==np.max(np.abs(eog_scores)):\n",
    "                ICA_to_drop=ii\n",
    "                break  \n",
    "    ica.exclude = [ii]\n",
    "    ica.apply(raw)\n",
    "    return(eog_indices, eog_scores)\n",
    "\n",
    "def get_epochs(raw,chosen_channels_mne,events,beg_f,end_f): \n",
    "    raw.filter(beg_f, end_f, fir_design='firwin', skip_by_annotation='edge')\n",
    "    event_id = dict(LH=1, RH=2) \n",
    "    tmin, tmax = .5,4.5\n",
    "    epochs = Epochs(raw, events, event_id, tmin, tmax, proj=True, picks=chosen_channels_mne, baseline=None, preload=False,reject=None) \n",
    "    epochs_data = epochs.get_data() \n",
    "    my_labels = epochs.events[:, -1] \n",
    "    return(epochs,my_labels,event_id,epochs_data)\n",
    "\n",
    "def csp_svm(epochs_data, my_labels): \n",
    "    gs=20 \n",
    "    for i in range(1,gs): \n",
    "        csp = CSP(n_components=i, reg=None, log=True, norm_trace=False) # n_components is chosen by grid search (by means of median accuracies of cross-validation) (grid search) \n",
    "        csp_transformed_data=csp.fit_transform(epochs_data,my_labels)\n",
    "\n",
    "        scores = []\n",
    "        cv = ShuffleSplit(10, test_size=0.25, random_state=42)\n",
    "        lda=LinearDiscriminantAnalysis()\n",
    "        clf =lda #svm.SVC(kernel='linear',C=100, gamma=.1)\n",
    "        scores = cross_val_score(clf, csp_transformed_data, my_labels, cv=cv, n_jobs=None)\n",
    "        #print(\"median score over cvs\",np.median(scores))\n",
    "        if i==1:\n",
    "            highest_median_score=np.median(scores)\n",
    "            chosen_number_comp=i\n",
    "        elif np.median(scores)>=highest_median_score:\n",
    "            highest_median_score=np.median(scores)\n",
    "            chosen_number_comp=i\n",
    "\n",
    "    csp = CSP(n_components=chosen_number_comp, reg=None, log=True, norm_trace=False)    \n",
    "    csp_transformed_data=csp.fit_transform(epochs_data,my_labels)\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(10, test_size=0.25, random_state=42)\n",
    "    lda=LinearDiscriminantAnalysis()\n",
    "    clf = lda \n",
    "    scores = cross_val_score(clf, csp_transformed_data, my_labels, cv=cv, n_jobs=None)\n",
    "    final_score=np.mean(scores)\n",
    "    return(final_score)\n",
    "\n",
    "my_scores_rhythm={}\n",
    "my_scores_run={}\n",
    "my_scores_sub={}\n",
    "for sub in range(1,needed_subjects+1):\n",
    "    num_runs=len(sub_dict[sub])\n",
    "    for run in range(1,num_runs+1):\n",
    "        data,events=sub_data[sub][run],sub_events[sub][run].astype(int)\n",
    "        data_scaled=data_scaling(data) \n",
    "        num_PCA_comp=find_PCA_components(data_scaled) \n",
    "        raw=define_raw(data_scaled)\n",
    "        eog_indices, eog_scores=ICA_components(raw,num_PCA_comp) \n",
    "        for rhythm in range(1,6):\n",
    "            if rhythm==1:\n",
    "                beg_f=7;end_f=15\n",
    "            elif rhythm==2:\n",
    "                beg_f=15;end_f=30\n",
    "            elif rhythm==3:\n",
    "                beg_f=30;end_f=100\n",
    "            elif rhythm==4:\n",
    "                beg_f=7;end_f=30\n",
    "            else:\n",
    "                beg_f=7;end_f=100\n",
    "            epochs,my_labels,event_id,epochs_data=get_epochs(raw,chosen_channels_mne,events,beg_f,end_f) \n",
    "            final_score=csp_svm(epochs_data, my_labels) \n",
    "            my_scores_rhythm[rhythm]=final_score\n",
    "        my_scores_run[run]=my_scores_rhythm\n",
    "        my_scores_rhythm={}\n",
    "    my_scores_sub[sub]=my_scores_run\n",
    "    my_scores_run={}\n",
    "    print(\"sub\",sub,\"is done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b03a278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
